{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d6d530-b250-405e-95c7-d00c0003efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative https://www.kaggle.com/code/hughhuyton/multitouch-attribution-modelling\n",
    "# https://pypi.org/project/ChannelAttribution/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e7a8a-935a-4edc-b5f0-10594cb5626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------beta 1.2------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sample dataset\n",
    "np.random.seed(42)\n",
    "n_customers = 10000\n",
    "n_interactions = np.random.randint(2, 16, size=n_customers)\n",
    "customer_ids = np.concatenate([np.repeat(customer_id, n) for customer_id, n in zip(np.arange(n_customers), n_interactions)])\n",
    "interaction_dates = np.concatenate([pd.date_range(start='2024-01-01', periods=n, freq='D') for n in n_interactions])\n",
    "channels = np.random.choice(['Email', 'Social Media', 'Paid Search', 'Direct', 'Organic Search'], len(customer_ids), p=[0.25, 0.25, 0.2, 0.15, 0.15])\n",
    "conversions = np.random.choice([0, 1], len(customer_ids), p=[0.9, 0.1])\n",
    "countries = np.random.choice(['USA', 'Canada', 'UK', 'Germany', 'France'], n_customers)\n",
    "\n",
    "# Repeat countries to match the number of interactions\n",
    "country_repeated = np.concatenate([np.repeat(country, n) for country, n in zip(countries, n_interactions)])\n",
    "\n",
    "data = {\n",
    "    'customer_id': customer_ids,\n",
    "    'interaction_date': interaction_dates,\n",
    "    'channel': channels,\n",
    "    'conversion': conversions,\n",
    "    'country': country_repeated\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort by customer_id and interaction_date\n",
    "df = df.sort_values(by=['customer_id', 'interaction_date'])\n",
    "print(df.tail())\n",
    "df.to_csv('lt.csv', index=False)\n",
    "\n",
    "def dda(country):\n",
    "    country_df = df[df['country'] == country]\n",
    "    if country_df.empty:\n",
    "        print(f\"No data for {country}. Skipping...\\n\")\n",
    "        return\n",
    "\n",
    "    # Print basic statistics to check interactions\n",
    "    print(f'\\n\\n\\n')\n",
    "    print(f'Total number of interactions in {country} dataframe: {len(country_df)}')\n",
    "    print(f'Number of unique customers: {country_df[\"customer_id\"].nunique()}')\n",
    "    print(f'Total number of conversions in original dataframe: {country_df[\"conversion\"].sum()}')\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "    # Encode the channels\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(country_df['channel'])\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "    # Add onehot_encoded channels to the dataframe using .loc\n",
    "    channel_columns = [f'channel_{i}' for i in range(onehot_encoded.shape[1])]\n",
    "    country_df.loc[:, channel_columns] = onehot_encoded\n",
    "\n",
    "    # Prepare sequences of touchpoints\n",
    "    sequences = []\n",
    "    sequence_labels = []\n",
    "    customer_ids_seq = []\n",
    "\n",
    "    for customer_id in country_df['customer_id'].unique():\n",
    "        customer_data = country_df[country_df['customer_id'] == customer_id]\n",
    "        sequences.append(customer_data[channel_columns].values)\n",
    "        sequence_labels.append(customer_data['conversion'].values)\n",
    "        customer_ids_seq.append(customer_id)\n",
    "\n",
    "    # Pad sequences to have the same length\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    X = pad_sequences(sequences, maxlen=max_sequence_length, dtype='float32', padding='post', value=0.0)\n",
    "    y = pad_sequences(sequence_labels, maxlen=max_sequence_length, dtype='float32', padding='post', value=0.0)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test, customer_ids_train, customer_ids_test = train_test_split(X, y, customer_ids_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(max_sequence_length, len(channel_columns)), return_sequences=True),\n",
    "        Dropout(0.5),\n",
    "        LSTM(32, return_sequences=True),\n",
    "        Dropout(0.5),\n",
    "        TimeDistributed(Dense(1, activation='linear'))\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Train MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f'Test Loss: {loss}, Test MAE: {mae}')\n",
    "\n",
    "    # Make predictions on the entire dataset\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Convert predictions to DataFrame with channels\n",
    "    predictions = []\n",
    "    for i in range(len(X)):\n",
    "        customer_id = customer_ids_seq[i]\n",
    "        for j in range(max_sequence_length):\n",
    "            if np.sum(X[i][j]) == 0:\n",
    "                continue\n",
    "            channel_index = np.argmax(X[i][j])\n",
    "            channel = label_encoder.inverse_transform([channel_index])[0]\n",
    "            predictions.append({\n",
    "                'customer_id': customer_id,\n",
    "                'interaction': j + 1,\n",
    "                'channel': channel,\n",
    "                'predicted_conversion_credit': y_pred[i][j][0]\n",
    "            })\n",
    "\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    print(predictions_df.head(10))\n",
    "\n",
    "    # Save predictions to a CSV file\n",
    "    predictions_df.to_csv(f'predictions_with_channels_{country}.csv', index=False)\n",
    "\n",
    "    # Display the results with channels and customer_id\n",
    "    print(predictions_df.head(10))\n",
    "\n",
    "    # Group by channel and sum the predicted conversion credits\n",
    "    channel_credit = predictions_df.groupby('channel')['predicted_conversion_credit'].sum().reset_index()\n",
    "\n",
    "    # Normalize the credits (optional)\n",
    "    channel_credit['normalized_credit'] = channel_credit['predicted_conversion_credit'] / channel_credit['predicted_conversion_credit'].sum()\n",
    "\n",
    "    channel_credit.to_csv(f'channel_credit_{country}.csv', index=False)\n",
    "\n",
    "    print(channel_credit)\n",
    "\n",
    "    # Additional checks\n",
    "    original_conversions = df['conversion'].sum()\n",
    "\n",
    "    print(f'Original conversions: {original_conversions}')\n",
    "    print(f'Number of unique customers in test set: {len(set(customer_ids_test))}')\n",
    "    print(f'Total number of interactions in test set: {len(customer_ids_test)}')\n",
    "    print(f'Total number of predictions: {len(predictions_df)}')\n",
    "\n",
    "country_list = ['Canada', 'France', 'Germany', 'UK', 'USA']\n",
    "\n",
    "for country in country_list:\n",
    "   dda(country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5badbd7b-bdc5-4473-ab72-1a57a818cd15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b795d15-4d8b-43bd-bba5-d27ee6d1be2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
